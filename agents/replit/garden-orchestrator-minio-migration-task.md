# üöÄ Replit Agent Task: Migrate Garden-Orchestrator from Cloudflare KV to MinIO

**–ú–µ—Ç–∞**: –ó–∞–º—ñ–Ω–∏—Ç–∏ Cloudflare KV storage –Ω–∞ MinIO –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è –≤–∏—Ç—Ä–∞—Ç —Ç–∞ —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –ª—ñ–º—ñ—Ç—ñ–≤ –±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ–≥–æ –ø–ª–∞–Ω—É CF.

**–î–∞—Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è**: 2026-01-17
**–ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç**: HIGH
**–°—Ç–∞—Ç—É—Å**: TODO

---

## üìã –ö–û–ù–¢–ï–ö–°–¢ –ü–†–û–ë–õ–ï–ú–ò

### –ü–æ—Ç–æ—á–Ω–∏–π —Å—Ç–∞–Ω
- **Cloudflare Worker** `garden-orchestrator` –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î **KV storage** –¥–ª—è:
  - –ß–µ—Ä–≥–∏ –∑–∞–≤–¥–∞–Ω—å (`queue:pending`)
  - –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–∞–≤–¥–∞–Ω—å (`task:{id}`)
  - –†–µ—î—Å—Ç—Ä–∞—Ü—ñ—ó –≤–æ—Ä–∫–µ—Ä—ñ–≤ (`worker:{id}`)
  - –Ü—Å—Ç–æ—Ä—ñ—ó –∑–∞–≤–µ—Ä—à–µ–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω—å (`completed:{id}`)
- **–ü—Ä–æ–±–ª–µ–º–∞**: –í–∏—á–µ—Ä–ø–∞–Ω–æ –¥–µ–Ω–Ω–∏–π –ª—ñ–º—ñ—Ç KV put() –æ–ø–µ—Ä–∞—Ü—ñ–π (Free tier: 1000 writes/day)
- **–ü—Ä–∏—á–∏–Ω–∞**: –ß–∞—Å—Ç—ñ polling-–∑–∞–ø–∏—Ç–∏ –≤—ñ–¥ RPi Worker + –±–∞–≥–∞—Ç–æ write –æ–ø–µ—Ä–∞—Ü—ñ–π –Ω–∞ –∫–æ–∂–Ω—É –¥—ñ—é

### –ü–æ—Ç–æ—á–Ω–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RPi Worker         ‚îÇ
‚îÇ  (Python, polling)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ HTTP (–∫–æ–∂–Ω—ñ 5-30 —Å–µ–∫)
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cloudflare Worker   ‚îÇ  ‚Üí Cloudflare KV (–õ–Ü–ú–Ü–¢!)
‚îÇ garden-orchestrator ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Lovable Frontend   ‚îÇ
‚îÇ  (task creation)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### –¶—ñ–ª—å–æ–≤–∞ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RPi Worker         ‚îÇ
‚îÇ  (Python, polling)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ HTTP (–∫–æ–∂–Ω—ñ 30-60 —Å–µ–∫)
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Replit FastAPI      ‚îÇ  ‚Üí MinIO Storage (NO LIMITS!)
‚îÇ garden-orchestrator ‚îÇ     + In-memory cache
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Lovable Frontend   ‚îÇ
‚îÇ  (task creation)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ –ó–ê–î–ê–ß–Ü

### Phase 1: MinIO Storage Service

#### 1.1 –°—Ç–≤–æ—Ä–∏—Ç–∏ MinIO –∫–ª—ñ—î–Ω—Ç

–§–∞–π–ª: `src/services/minio_storage.py`

```python
"""
MinIO Storage Service for Garden Orchestrator
Replaces Cloudflare KV with S3-compatible object storage
"""

import json
import os
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
from minio import Minio
from minio.error import S3Error
from io import BytesIO
import asyncio
from functools import lru_cache


class MinioStorageService:
    """
    S3-compatible storage using MinIO for task queue management.
    Replaces Cloudflare KV to avoid rate limits.
    """
    
    def __init__(self):
        self.client = Minio(
            os.getenv("MINIO_ENDPOINT", "localhost:9000"),
            access_key=os.getenv("MINIO_ACCESS_KEY", ""),
            secret_key=os.getenv("MINIO_SECRET_KEY", ""),
            secure=os.getenv("MINIO_SECURE", "true").lower() == "true"
        )
        self.bucket = os.getenv("MINIO_BUCKET", "garden-orchestrator")
        
        # In-memory cache to reduce S3 calls
        self._cache: Dict[str, tuple] = {}  # key -> (value, expires_at)
        self._cache_ttl = 10  # seconds
        
        self._ensure_bucket()
    
    def _ensure_bucket(self):
        """Create bucket if not exists"""
        try:
            if not self.client.bucket_exists(self.bucket):
                self.client.make_bucket(self.bucket)
        except S3Error as e:
            print(f"MinIO bucket error: {e}")
    
    # ==================== CACHE LAYER ====================
    
    def _cache_get(self, key: str) -> Optional[Any]:
        """Get from cache if not expired"""
        if key in self._cache:
            value, expires_at = self._cache[key]
            if datetime.utcnow() < expires_at:
                return value
            del self._cache[key]
        return None
    
    def _cache_set(self, key: str, value: Any, ttl: int = None):
        """Set cache with TTL"""
        ttl = ttl or self._cache_ttl
        self._cache[key] = (value, datetime.utcnow() + timedelta(seconds=ttl))
    
    def _cache_delete(self, key: str):
        """Remove from cache"""
        self._cache.pop(key, None)
    
    # ==================== STORAGE OPERATIONS ====================
    
    def put(self, key: str, data: Dict[str, Any], ttl: Optional[int] = None) -> bool:
        """
        Store JSON data in MinIO.
        
        Args:
            key: Storage key (e.g., "task:abc123")
            data: Dictionary to store
            ttl: Time-to-live in seconds (stored as metadata, cleanup handled separately)
        """
        try:
            json_bytes = json.dumps(data).encode('utf-8')
            stream = BytesIO(json_bytes)
            
            metadata = {
                "created_at": datetime.utcnow().isoformat(),
            }
            if ttl:
                metadata["expires_at"] = (datetime.utcnow() + timedelta(seconds=ttl)).isoformat()
            
            self.client.put_object(
                self.bucket,
                key,
                stream,
                len(json_bytes),
                content_type="application/json",
                metadata=metadata
            )
            
            # Update cache
            self._cache_set(key, data)
            return True
            
        except S3Error as e:
            print(f"MinIO put error: {e}")
            return False
    
    def get(self, key: str) -> Optional[Dict[str, Any]]:
        """
        Retrieve JSON data from MinIO.
        Uses cache to reduce S3 operations.
        """
        # Check cache first
        cached = self._cache_get(key)
        if cached is not None:
            return cached
        
        try:
            response = self.client.get_object(self.bucket, key)
            data = json.loads(response.read().decode('utf-8'))
            response.close()
            response.release_conn()
            
            # Cache the result
            self._cache_set(key, data)
            return data
            
        except S3Error as e:
            if e.code != "NoSuchKey":
                print(f"MinIO get error: {e}")
            return None
    
    def delete(self, key: str) -> bool:
        """Delete object from MinIO"""
        try:
            self.client.remove_object(self.bucket, key)
            self._cache_delete(key)
            return True
        except S3Error as e:
            print(f"MinIO delete error: {e}")
            return False
    
    def list_keys(self, prefix: str) -> List[str]:
        """List all keys with prefix"""
        try:
            objects = self.client.list_objects(self.bucket, prefix=prefix)
            return [obj.object_name for obj in objects]
        except S3Error as e:
            print(f"MinIO list error: {e}")
            return []
    
    # ==================== BATCH OPERATIONS ====================
    
    def get_many(self, keys: List[str]) -> Dict[str, Any]:
        """Get multiple objects at once (with caching)"""
        result = {}
        keys_to_fetch = []
        
        # Check cache first
        for key in keys:
            cached = self._cache_get(key)
            if cached is not None:
                result[key] = cached
            else:
                keys_to_fetch.append(key)
        
        # Fetch remaining from MinIO
        for key in keys_to_fetch:
            data = self.get(key)
            if data:
                result[key] = data
        
        return result
    
    # ==================== CLEANUP ====================
    
    async def cleanup_expired(self):
        """Remove expired objects (run periodically)"""
        now = datetime.utcnow()
        try:
            for prefix in ["completed:", "worker:"]:
                objects = self.client.list_objects(self.bucket, prefix=prefix)
                for obj in objects:
                    stat = self.client.stat_object(self.bucket, obj.object_name)
                    expires_at = stat.metadata.get("x-amz-meta-expires_at")
                    if expires_at:
                        expire_time = datetime.fromisoformat(expires_at)
                        if now > expire_time:
                            self.delete(obj.object_name)
        except S3Error as e:
            print(f"Cleanup error: {e}")


# Singleton instance
@lru_cache(maxsize=1)
def get_minio_storage() -> MinioStorageService:
    return MinioStorageService()
```

#### 1.2 –û–Ω–æ–≤–∏—Ç–∏ Task Queue –¥–ª—è MinIO

–§–∞–π–ª: `src/services/task_queue_minio.py`

```python
"""
Task Queue Service using MinIO storage
Replaces in-memory queue with persistent MinIO-backed storage
"""

import uuid
from datetime import datetime
from typing import Optional, List, Dict, Any
from src.services.minio_storage import get_minio_storage


class TaskQueueMinio:
    """
    Persistent task queue backed by MinIO storage.
    Optimized to minimize storage operations.
    """
    
    QUEUE_KEY = "queue:pending"
    
    def __init__(self):
        self.storage = get_minio_storage()
        # In-memory queue cache (primary source, synced to MinIO)
        self._queue_cache: Optional[List[Dict]] = None
        self._queue_dirty = False
    
    # ==================== QUEUE OPERATIONS ====================
    
    def _load_queue(self) -> List[Dict]:
        """Load queue from storage (with caching)"""
        if self._queue_cache is not None:
            return self._queue_cache
        
        data = self.storage.get(self.QUEUE_KEY)
        self._queue_cache = data if data else []
        return self._queue_cache
    
    def _save_queue(self):
        """Save queue to storage (batched)"""
        if self._queue_cache is not None:
            self.storage.put(self.QUEUE_KEY, self._queue_cache)
            self._queue_dirty = False
    
    def _mark_dirty(self):
        """Mark queue as needing sync"""
        self._queue_dirty = True
    
    def sync(self):
        """Sync queue to storage if dirty"""
        if self._queue_dirty:
            self._save_queue()
    
    # ==================== TASK OPERATIONS ====================
    
    def create_task(self, task_data: Dict[str, Any]) -> str:
        """Create a new task"""
        task_id = str(uuid.uuid4())
        
        task = {
            "id": task_id,
            "role": task_data.get("role", "archivist"),
            "task_type": task_data.get("task_type"),
            "input_data": task_data.get("input_data", {}),
            "priority": task_data.get("priority", 5),
            "status": "pending",
            "created_at": datetime.utcnow().isoformat(),
            "updated_at": datetime.utcnow().isoformat(),
            "project_id": task_data.get("project_id", "default"),
            "context_enabled": task_data.get("context_enabled", True),
            "context_limit": task_data.get("context_limit", 10),
        }
        
        # Save task
        self.storage.put(f"task:{task_id}", task)
        
        # Add to queue (with metadata for filtering)
        queue = self._load_queue()
        queue.append({
            "id": task_id,
            "priority": task["priority"],
            "created_at": task["created_at"],
            "role": task["role"],
            "status": "pending"
        })
        
        # Sort by priority (desc) then created_at (asc)
        queue.sort(key=lambda x: (-x["priority"], x["created_at"]))
        self._queue_cache = queue
        self._save_queue()  # Immediate save for new tasks
        
        return task_id
    
    def get_task(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get task by ID"""
        return self.storage.get(f"task:{task_id}")
    
    def get_next_task(self, worker_id: str, roles: List[str] = None) -> Optional[Dict[str, Any]]:
        """Get and assign next pending task for worker"""
        queue = self._load_queue()
        
        for i, item in enumerate(queue):
            if item.get("status") != "pending":
                continue
            if roles and item.get("role") not in roles:
                continue
            
            # Get full task
            task = self.get_task(item["id"])
            if not task or task.get("status") != "pending":
                continue
            
            # Assign task
            task["status"] = "processing"
            task["assigned_to"] = worker_id
            task["assigned_at"] = datetime.utcnow().isoformat()
            task["updated_at"] = datetime.utcnow().isoformat()
            self.storage.put(f"task:{item['id']}", task)
            
            # Remove from queue
            queue.pop(i)
            self._queue_cache = queue
            self._save_queue()
            
            return task
        
        return None
    
    def complete_task(self, task_id: str, result: Any, status: str = "completed", 
                      observation_id: str = None) -> bool:
        """Mark task as completed"""
        task = self.get_task(task_id)
        if not task:
            return False
        
        task["status"] = status
        task["result"] = result
        task["completed_at"] = datetime.utcnow().isoformat()
        task["updated_at"] = datetime.utcnow().isoformat()
        if observation_id:
            task["observation_id"] = observation_id
        
        # Save task
        self.storage.put(f"task:{task_id}", task)
        
        # Save to completed history (with TTL metadata)
        self.storage.put(f"completed:{task_id}", task, ttl=86400 * 7)  # 7 days
        
        return True
    
    def list_pending_tasks(self, limit: int = 50) -> List[Dict[str, Any]]:
        """List pending tasks (queue metadata only, no full task fetch)"""
        queue = self._load_queue()
        return queue[:limit]
    
    def list_tasks_full(self, limit: int = 50) -> List[Dict[str, Any]]:
        """List pending tasks with full data"""
        queue = self._load_queue()
        task_ids = [f"task:{item['id']}" for item in queue[:limit]]
        tasks_data = self.storage.get_many(task_ids)
        return [tasks_data.get(f"task:{item['id']}") for item in queue[:limit] 
                if tasks_data.get(f"task:{item['id']}")]
    
    # ==================== WORKER OPERATIONS ====================
    
    def register_worker(self, worker_id: str, capabilities: List[str] = None) -> bool:
        """Register a worker"""
        worker = {
            "worker_id": worker_id,
            "capabilities": capabilities or [],
            "registered_at": datetime.utcnow().isoformat(),
            "last_heartbeat": datetime.utcnow().isoformat(),
            "status": "active",
        }
        return self.storage.put(f"worker:{worker_id}", worker, ttl=600)  # 10 min TTL
    
    def worker_heartbeat(self, worker_id: str) -> bool:
        """Update worker heartbeat"""
        worker = self.storage.get(f"worker:{worker_id}")
        if worker:
            worker["last_heartbeat"] = datetime.utcnow().isoformat()
            return self.storage.put(f"worker:{worker_id}", worker, ttl=600)
        return False
    
    def list_workers(self) -> List[Dict[str, Any]]:
        """List active workers"""
        worker_keys = self.storage.list_keys("worker:")
        workers_data = self.storage.get_many(worker_keys)
        return list(workers_data.values())
    
    # ==================== STATS ====================
    
    def get_stats(self) -> Dict[str, Any]:
        """Get queue statistics"""
        queue = self._load_queue()
        workers = self.list_workers()
        
        return {
            "pending_tasks": len(queue),
            "active_workers": len(workers),
            "version": "3.0-minio",
            "storage": "minio",
            "claude_mem_enabled": True,
        }


# Singleton
_task_queue_instance: Optional[TaskQueueMinio] = None

def get_task_queue() -> TaskQueueMinio:
    global _task_queue_instance
    if _task_queue_instance is None:
        _task_queue_instance = TaskQueueMinio()
    return _task_queue_instance
```

### Phase 2: –û–Ω–æ–≤–∏—Ç–∏ FastAPI endpoints

#### 2.1 –ù–æ–≤–∏–π main.py –∑ MinIO

–§–∞–π–ª: `src/main_minio.py`

```python
"""
Garden AI Orchestrator - FastAPI with MinIO Storage
Replaces Cloudflare Worker to avoid KV limits
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager
import asyncio

from src.services.task_queue_minio import get_task_queue


# ==================== MODELS ====================

class TaskCreate(BaseModel):
    role: str = "archivist"
    task_type: str
    input_data: Dict[str, Any] = Field(default_factory=dict)
    priority: int = 5
    project_id: str = "default"
    context_enabled: bool = True
    context_limit: int = 10

class TaskComplete(BaseModel):
    task_id: str
    result: Any = None
    status: str = "completed"
    observation_id: Optional[str] = None

class WorkerRegister(BaseModel):
    worker_id: str
    capabilities: List[str] = Field(default_factory=list)

class WorkerHeartbeat(BaseModel):
    worker_id: str


# ==================== APP ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: periodic cleanup task
    async def cleanup_loop():
        while True:
            await asyncio.sleep(3600)  # Every hour
            await get_task_queue().storage.cleanup_expired()
    
    task = asyncio.create_task(cleanup_loop())
    yield
    task.cancel()


app = FastAPI(
    title="Garden AI Orchestrator",
    version="3.0-minio",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== ENDPOINTS ====================

@app.get("/health")
async def health():
    return {
        "status": "ok",
        "service": "garden-orchestrator",
        "version": "3.0-minio",
        "storage": "minio"
    }


@app.get("/stats")
async def stats():
    return get_task_queue().get_stats()


# ---- Task endpoints ----

@app.post("/tasks")
async def create_task(task: TaskCreate):
    task_id = get_task_queue().create_task(task.model_dump())
    return {"task_id": task_id, "status": "created"}


@app.get("/tasks/{task_id}")
async def get_task(task_id: str):
    task = get_task_queue().get_task(task_id)
    if not task:
        raise HTTPException(404, "Task not found")
    return task


@app.get("/tasks")
async def list_tasks():
    tasks = get_task_queue().list_tasks_full()
    return {"tasks": tasks}


# ---- Polling endpoints (for RPi) ----

@app.post("/poll/register")
async def register_worker(data: WorkerRegister):
    get_task_queue().register_worker(data.worker_id, data.capabilities)
    return {"status": "registered", "worker_id": data.worker_id}


@app.post("/poll/heartbeat")
async def worker_heartbeat(data: WorkerHeartbeat):
    success = get_task_queue().worker_heartbeat(data.worker_id)
    return {"status": "ok" if success else "not_found"}


@app.get("/poll/next")
async def get_next_task(worker_id: str, roles: str = ""):
    roles_list = [r.strip() for r in roles.split(",") if r.strip()] if roles else None
    task = get_task_queue().get_next_task(worker_id, roles_list)
    
    if task:
        return {"task": task}
    return {
        "task": None, 
        "message": "No tasks available",
        "retry_after": 30  # Hint for client to wait
    }


@app.post("/poll/complete")
async def complete_task(data: TaskComplete):
    success = get_task_queue().complete_task(
        data.task_id, 
        data.result, 
        data.status,
        data.observation_id
    )
    if not success:
        raise HTTPException(404, "Task not found")
    return {"status": "completed", "task_id": data.task_id}


@app.get("/poll/workers")
async def list_workers():
    workers = get_task_queue().list_workers()
    return {"workers": workers}


# ==================== RUN ====================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=5000)
```

### Phase 3: –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è

#### 3.1 Environment variables

–§–∞–π–ª: `.env.example` (–æ–Ω–æ–≤–∏—Ç–∏)

```bash
# MinIO Configuration (S3-compatible)
MINIO_ENDPOINT=https://apiminio.exodus.pp.ua
MINIO_ACCESS_KEY=your_access_key
MINIO_SECRET_KEY=your_secret_key
MINIO_SECURE=true
MINIO_BUCKET=garden-orchestrator

# Alternative: Use existing MinIO from your infrastructure
# MINIO_ENDPOINT=minio.exodus.pp.ua

# Claude-Mem (unchanged)
CLAUDE_MEM_ENABLED=true
CLAUDE_MEM_DB_PATH=~/.claude-mem/claude-mem.db
CLAUDE_MEM_CONTEXT_LIMIT=10

# FastAPI
PORT=5000
HOST=0.0.0.0
```

#### 3.2 –û–Ω–æ–≤–∏—Ç–∏ requirements.txt

```txt
fastapi>=0.100.0
uvicorn>=0.23.0
pydantic>=2.0.0
pydantic-settings>=2.0.0
httpx>=0.24.0
numpy>=1.24.0
python-multipart>=0.0.6
minio>=7.2.0
```

---

## üîß –û–ü–¢–ò–ú–Ü–ó–ê–¶–Ü–á (–í–ê–ñ–õ–ò–í–û!)

### 1. –ó–º–µ–Ω—à–µ–Ω–Ω—è –æ–ø–µ—Ä–∞—Ü—ñ–π –∑–∞–ø–∏—Å—É

| –û–ø–µ—Ä–∞—Ü—ñ—è | –ë—É–ª–æ (CF KV) | –°—Ç–∞–ª–æ (MinIO + cache) |
|----------|--------------|----------------------|
| Create task | 2 writes (task + queue) | 2 writes (batched) |
| Get next task | 2 writes + N reads | 1 read (cached queue) + 2 writes |
| Heartbeat | 1 write –∫–æ–∂–Ω—ñ 5 —Å–µ–∫ | 1 write –∫–æ–∂–Ω—ñ 30-60 —Å–µ–∫ |
| List tasks | N reads | 1 read (cached) |

### 2. RPi Worker polling

**–û–Ω–æ–≤–∏—Ç–∏** `worker.py` –Ω–∞ RPi:

```python
# –ë—É–ª–æ
POLL_INTERVAL = 5  # —Å–µ–∫—É–Ω–¥

# –°—Ç–∞–ª–æ
POLL_INTERVAL = 30  # —Å–µ–∫—É–Ω–¥ (–∞–±–æ 60 —è–∫—â–æ –Ω–µ–º–∞—î –∞–∫—Ç–∏–≤–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω—å)
POLL_INTERVAL_IDLE = 60  # —è–∫—â–æ –Ω–µ–º–∞—î –∑–∞–≤–¥–∞–Ω—å
```

### 3. In-memory cache

- Queue –∫–µ—à—É—î—Ç—å—Å—è –≤ –ø–∞–º'—è—Ç—ñ, —Å–∏–Ω—Ö—Ä–æ–Ω—ñ–∑—É—î—Ç—å—Å—è –∑ MinIO –ª–∏—à–µ –ø—Ä–∏ –∑–º—ñ–Ω–∞—Ö
- –ó–∞–≤–¥–∞–Ω–Ω—è –∫–µ—à—É—é—Ç—å—Å—è –Ω–∞ 10 —Å–µ–∫—É–Ω–¥
- Workers –∫–µ—à—É—é—Ç—å—Å—è –Ω–∞ 30 —Å–µ–∫—É–Ω–¥

---

## üìä –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø

| –ú–µ—Ç—Ä–∏–∫–∞ | Cloudflare KV (Free) | MinIO (Self-hosted) |
|---------|---------------------|---------------------|
| Write limit | 1000/day | ‚àû (unlimited) |
| Read limit | 100,000/day | ‚àû (unlimited) |
| Storage | 1GB | Your disk |
| Latency | ~50ms (edge) | ~10ms (local) |
| Cost | Free ‚Üí $5+/mo | Free (self-hosted) |

---

## ‚úÖ CHECKLIST

### Phase 1: Storage
- [ ] –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ `minio` package
- [ ] –°—Ç–≤–æ—Ä–∏—Ç–∏ `src/services/minio_storage.py`
- [ ] –ù–∞–ª–∞—à—Ç—É–≤–∞—Ç–∏ MinIO bucket `garden-orchestrator`
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç—É–≤–∞—Ç–∏ CRUD –æ–ø–µ—Ä–∞—Ü—ñ—ó

### Phase 2: Task Queue
- [ ] –°—Ç–≤–æ—Ä–∏—Ç–∏ `src/services/task_queue_minio.py`
- [ ] –Ü–º–ø–ª–µ–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –∫–µ—à—É–≤–∞–Ω–Ω—è —á–µ—Ä–≥–∏
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç—É–≤–∞—Ç–∏ create/get/complete flow

### Phase 3: API
- [ ] –û–Ω–æ–≤–∏—Ç–∏ `src/main.py` –∞–±–æ —Å—Ç–≤–æ—Ä–∏—Ç–∏ `src/main_minio.py`
- [ ] –î–æ–¥–∞—Ç–∏ CORS middleware
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç—É–≤–∞—Ç–∏ –≤—Å—ñ endpoints

### Phase 4: Migration
- [ ] –ó—É–ø–∏–Ω–∏—Ç–∏ Cloudflare Worker
- [ ] –û–Ω–æ–≤–∏—Ç–∏ RPi Worker URL –Ω–∞ Replit
- [ ] –û–Ω–æ–≤–∏—Ç–∏ Lovable Frontend URL
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç—É–≤–∞—Ç–∏ end-to-end

### Phase 5: Cleanup
- [ ] –í–∏–¥–∞–ª–∏—Ç–∏ —Å—Ç–∞—Ä—É –ª–æ–≥—ñ–∫—É CF KV
- [ ] –û–Ω–æ–≤–∏—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—é
- [ ] –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ MinIO usage

---

## üöÄ –ö–û–ú–ê–ù–î–ò –¢–ï–°–¢–£–í–ê–ù–ù–Ø

```bash
# Health check
curl http://localhost:5000/health

# Stats
curl http://localhost:5000/stats

# Create task
curl -X POST http://localhost:5000/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "role": "archivist",
    "task_type": "summarize",
    "input_data": {"text": "Test task"},
    "project_id": "test"
  }'

# Get next task (as worker)
curl "http://localhost:5000/poll/next?worker_id=test-worker&roles=archivist"

# Complete task
curl -X POST http://localhost:5000/poll/complete \
  -H "Content-Type: application/json" \
  -d '{"task_id": "YOUR_TASK_ID", "result": {"output": "Done"}}'
```

---

## üìù NOTES

- MinIO –º–∞—î –±—É—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω–∏–π –∑ Replit (–ø—É–±–ª—ñ—á–Ω–∏–π endpoint –∞–±–æ VPN)
- –Ø–∫—â–æ MinIO –Ω–∞ exodus.pp.ua, –ø–µ—Ä–µ–∫–æ–Ω–∞–π—Å—è —â–æ –ø–æ—Ä—Ç 9000 –≤—ñ–¥–∫—Ä–∏—Ç–∏–π
- –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ Supabase Storage –∞–±–æ R2 (Cloudflare S3-compatible)
- –ü—Ä–∏ –ø–µ—Ä–µ—Ö–æ–¥—ñ - –∑—Ä–æ–±–∏ backup –ø–æ—Ç–æ—á–Ω–∏—Ö –∑–∞–≤–¥–∞–Ω—å –∑ CF KV

---

**–ê–≤—Ç–æ—Ä**: Lovable AI Agent
**–î–ª—è**: Replit Agent
**–ü—Ä–æ—î–∫—Ç**: Garden-Agent-Service
